---
title: "InteligÃªncia Artificial Generativa e Pesquisa nas Humanidades"
subtitle: "Ã‰tica, epistemologia e prÃ¡ticas locais com Ollama\br[![DOI](https://zenodo.org/badge/1103143805.svg)](https://doi.org/10.5281/zenodo.17736234)"
date: 2025-11-26
date-format: full
lang: pt-br
format:
  revealjs:
    theme: [default, custom.scss]
    slide-number: true
    incremental: false
    chalkboard:
      buttons: true
    footer: "Eric Brasil | <a href='https://ericbrasil.com.br/contact/'>Entre em contato</a> | 2Âª Semana de Humanidades Digitais da UFBA"
    logo: imgs/logo.png
author:
  - name: Eric Brasil
    orcid: 0000-0001-5067-8475
    affiliation: (UNILAB â€¢ PPGIHD/UFRRJ â€¢ LABHDUFBA)
description: "Oficina prÃ¡tica apresentada na 2Âª Semana de Humanidades Digitais da UFBA. A atividade aborda fundamentos, implicaÃ§Ãµes Ã©ticas e epistemolÃ³gicas e prÃ¡ticas locais com modelos abertos via Ollama, pensando a pesquisa em Humanidades em mÃ¡quinas pessoais e com software livre."
toc: false
---

## {.center}

![Acesse os slides da oficina em [https://ericbrasil.com.br/2shdufba/](https://ericbrasil.com.br/2shdufba/)](imgs/oficina.png)

---

## Nota sobre o uso de IA Generativa {.center}

ğŸ› ï¸ Esta apresentaÃ§Ã£o foi **produzida com o apoio do ChatGPT (modelo GPT-5)**, entre os dias **23 e 24 de novembro de 2025**, a partir de instruÃ§Ãµes e curadoria do professor **Eric Brasil** para a oficina **â€œInteligÃªncia Artificial Generativa e Pesquisa nas Humanidadesâ€**, realizada na **2Âª Semana de Humanidades Digitais da UFBA**.  

ğŸ¤– Todo o conteÃºdo, formataÃ§Ã£o e exemplos prÃ¡ticos foram gerados de forma **colaborativa**, com foco em **modelos abertos e execuÃ§Ã£o local via Ollama**, preservando as escolhas **editoriais, metodolÃ³gicas e pedagÃ³gicas** do autor.  

---

## Estrutura da Oficina {.columns}
<br>

### Objetivos  

::: {.column width="46%"}
- Compreender fundamentos conceituais da IA nas Humanidades.  
- Discutir dimensÃµes Ã©ticas, filosÃ³ficas e epistemolÃ³gicas.  

:::

::: {.column width="8%"}
:::
::: {.column width="46%"}
- Experimentar modelos **leves**, **abertos** e **locais** com **Ollama**.  
- Refletir sobre prÃ¡ticas responsÃ¡veis de uso de IA na pesquisa.  
:::

---

## Estrutura da Oficina {.columns}
<br>

### Estrutura  

::: {.column width="46%"}
1. **InteligÃªncia Artificial?**  
   FiccÃ§Ã£o cientÃ­fica, filosofia e ciÃªncia.  
2. **LLM: o que Ã©?**  
   Funcionamento, limites e potencial.  

:::

::: {.column width="8%"}
:::

::: {.column width="46%"}
3. **Rodando modelos locais com Ollama**  
    PrÃ¡tica com modelos abertos e criaÃ§Ã£o de Modelfiles.
:::

---

## 1. **InteligÃªncia Artificial?** FiccÃ§Ã£o cientÃ­fica, filosofia e ciÃªncia. {.center}

---

## O que Ã© InteligÃªncia Artificial? {.columns}
<br>

::: {.column width="40%"}
![](https://ericbrasil.com.br/cclhm00114/aulas/assets/IA-BOT_ChatGPT_20250410_091806.png)
:::

::: {.column width="60%"}
<br>
<br>

>ğŸ’¬ AlguÃ©m arrisca uma definiÃ§Ã£o?

:::

---

## Russel e Norvig (2004): quatro abordagens {.center}

ğŸ§  **Pensar como seres humanos**  

>â€œMÃ¡quinas com mentes, no sentido total e literal.â€ Haugeland (1985)

ğŸ¤– **Agir como seres humanos**  

>â€œA arte de criar mÃ¡quinas que executam funÃ§Ãµes que exigem inteligÃªncia quando executadas por pessoas.â€  Kurzweil (1990)

---

## Russel e Norvig (2004): quatro abordagens {.center}

ğŸ§ âš™ï¸ **Pensar racionalmente**  

>â€œEstudo das computaÃ§Ãµes que tornam possÃ­vel perceber, raciocinar e agir.â€ Winston (1992)

âš™ï¸ **Agir racionalmente**  

>â€œEstudo do projeto de agentes inteligentes.â€ Poole et al. (1998)

---

## Russel e Norvig (2004): quatro abordagens {.center}

ğŸ“Œ **As abordagens se dividem entre:**

- ğŸ§ª Enfoques empÃ­ricos e humanos (interdisciplinares)  
- ğŸ”¢ Enfoques lÃ³gico-racionalistas (computaÃ§Ã£o, matemÃ¡tica)

---

## Alan Turing e a pergunta fundamental {.center}

> â€œCan machines think?â€ [*Computing Machinery and Intelligence* (1950)](https://doi.org/10.1093/mind/LIX.236.433)

- Essa pergunta Ã© substituÃ­da por outra:  
  **Uma mÃ¡quina pode imitar um ser humano a ponto de enganar um observador?**

- Essa proposta dÃ¡ origem ao que hoje chamamos de **Teste de Turing**.

---

## Quem foi Alan Turing? {.columns}

::: {.column width="70%"}

- **Alan Mathison Turing** (1912â€“1954)  
- MatemÃ¡tico britÃ¢nico, pioneiro na computaÃ§Ã£o;
- Criou o conceito de **mÃ¡quina universal** â€“ base dos computadores;
- Atuou no projeto **Enigma**, quebrando cÃ³digos nazistas;
- PropÃ´s o **Teste de Turing** (Jogo da ImitaÃ§Ã£o, 1950);
:::

::: {.column width="30%"}
![Alan Turing aos 16 anos.](https://upload.wikimedia.org/wikipedia/commons/a/a1/Alan_Turing_Aged_16.jpg)
:::

---

## PerseguiÃ§Ã£o estatal {.center}

- Em 1952, Turing foi condenado por **homossexualidade**, entÃ£o considerada crime no Reino Unido;
- Foi submetido Ã  **castraÃ§Ã£o quÃ­mica** como pena alternativa Ã  prisÃ£o;
- Morreu em 1954, em circunstÃ¢ncias interpretadas como **suicÃ­dio**.

---

## ğŸ³ï¸â€ğŸŒˆ **Reconhecimento pÃ³stumo** {.columns}

::: {.column width="60%"}

- Em 2009, o governo britÃ¢nico emitiu um pedido pÃºblico de desculpas;
- Em 2013, recebeu **perdÃ£o pÃ³stumo oficial** da Rainha Elizabeth II;
- Hoje Ã© sÃ­mbolo da luta por direitos LGBTQIA+ e liberdade cientÃ­fica.
:::

::: {.column width="40%"}
![EstÃ¡tua de Alan Turing](https://upload.wikimedia.org/wikipedia/commons/1/1a/Alan_Turing_by_Stephen_Kettle_2007.jpg)
:::

---

## O Jogo da ImitaÃ§Ã£o {.center}

ğŸ® **TrÃªs participantes**:

- A: um homem  
- B: uma mulher  
- C: um interrogador (humano), isolado dos demais

âœ‰ï¸ **ComunicaÃ§Ã£o apenas por texto.**  
C tenta descobrir quem Ã© A e quem Ã© B.

---

## A mÃ¡quina entra em cena {.center}

ğŸ§  SubstituÃ­mos um dos humanos (A ou B) por uma **mÃ¡quina**.

â“ O interrogador consegue descobrir quem Ã© a mÃ¡quina?

ğŸ“Œ Se **nÃ£o consegue distinguir** com frequÃªncia maior que o acaso...

> ... entÃ£o **a mÃ¡quina â€œpensaâ€** â€” no sentido funcional proposto por Turing.

---

## ImportÃ¢ncia do Jogo da ImitaÃ§Ã£o {.center}

âœ… Evita debates abstratos sobre â€œconsciÃªnciaâ€ ou â€œalmaâ€  
âœ… Foca em **comportamento observÃ¡vel**  
âœ… Ã‰ um marco na histÃ³ria da inteligÃªncia artificial

> "Estamos interessados nÃ£o no que a mÃ¡quina **Ã©**,  
mas **no que ela faz**." Turing (1950)

---

### O teste Voight-Kampff em Blade Runner (dir. Ridley Scott, 1982) {.center}

![Utilizado para identificar **replicantes**, distingue humanos de mÃ¡quinas pela **capacidade empÃ¡tica**](https://ericbrasil.com.br/cclhm00114/aulas/assets/bladerunner.gif)

:::notes
- Utilizado para identificar **replicantes** (androides avanÃ§ados);
- Mede **reaÃ§Ãµes emocionais** (expressÃµes faciais, dilataÃ§Ã£o da pupila, etc.);
- Distingue humanos de mÃ¡quinas pela **capacidade empÃ¡tica**;
- Foco: **resposta afetiva e moral**.
:::

---

### Her (2013), dir. Spike Jonze {.center}

<img src="https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExMDNpbjV2cnZ2MmwydXNkZDc0ZHVtNGtrZTB4YnFxZXlwbzRvdXBqayZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/unyYJeETm09laXfGui/giphy.gif" width="80%" />

> ğŸ§  O que significa â€œamarâ€ uma IA?  
>
> AtÃ© que ponto uma consciÃªncia artificial pode ser considerada uma *pessoa*?

---

### Ex Machina (2014), dir. Alex Garland {.center}

![](https://ericbrasil.com.br/cclhm00114/aulas/assets/gif_ava.gif)

> ğŸ‘ï¸ A IA passa no Teste de Turing â€” mas ela estÃ¡ realmente *consciente*?  
>
>  â¤ï¸O afeto Ã© um critÃ©rio vÃ¡lido para reconhecer inteligÃªncia?

---

## Pensar a partir da ficÃ§Ã£o cientÃ­fica {.columns}

<br>

::: {.column width="40%"}
![](https://ericbrasil.com.br/cclhm00114/aulas/assets/IA-BOT_ChatGPT_20250410_091806.png)
:::

::: {.column width="60%"}
<br>

> ğŸ’¬ As ideias de Turing sobre pensamento artificial ainda fazem sentido diante das questÃµes levantadas pela ficÃ§Ã£o cientÃ­fica?

:::

---

## 2.**LLM: o que Ã©?** Funcionamento, limites e potencial. {.center}

---

## IA Generativa â‰  IA Geral {.center}

ğŸ¤– **IA Generativa (IAG)**  

- Especializada em criaÃ§Ã£o de conteÃºdos  
- Usa padrÃµes aprendidos em grandes volumes de dados  
- NÃ£o tem consciÃªncia nem compreensÃ£o

ğŸ§  **IA Geral (AGI)**  

- Habilidade de aprender qualquer tarefa cognitiva  
- Equivalente (ou superior) Ã  inteligÃªncia humana  
- Ainda Ã© **teÃ³rica**

---

## E a Singularidade? {.center}

ğŸŒŒ **Singularidade TecnolÃ³gica**  

- Futuro hipotÃ©tico onde a IA supera amplamente a inteligÃªncia humana  
- MudanÃ§as imprevisÃ­veis e irreversÃ­veis na sociedade

âš ï¸ NÃ£o Ã© consenso entre especialistas  
âš ï¸ Mais presente em narrativas de ficÃ§Ã£o e futurologia

---

## {.center}

![IA Generativa, IA Geral e Singularidade ](https://ericbrasil.com.br/cclhm00114/aulas/assets/HQ_ChatGPT_20250410_103434.png)

---
## InteligÃªncia Artificial Generativa em 2025 {.center}

A IA Generativa Ã© um ramo da IA capaz de criar conteÃºdos novos, como:

- ğŸ“ Textos
- ğŸ¨ Imagens
- ğŸ—£ï¸ Ãudio
- ğŸ“¹ VÃ­deo
- ğŸ§  CÃ³digos de software

---

## CaracterÃ­sticas Gerais da IAG {.center}

- Baseia-se em modelos avanÃ§ados que identificam padrÃµes em grandes volumes de dados.
- Aprende com dados massivos
- Produz conteÃºdo original e coerente
- Funciona com base em **probabilidades**, sem consciÃªncia
- Pode **alucinar** (gerar erros ou invenÃ§Ãµes convincentes)
- Requer **validaÃ§Ã£o humana constante**

---

## O que sÃ£o LLMs? {.center}

ğŸ“š **LLM** = *Large Language Model* (Modelo de Linguagem de Grande Escala)

SÃ£o redes neurais treinadas com bilhÃµes de palavras para:

- Entender e gerar linguagem natural
- Realizar tarefas complexas de texto (resumos, respostas, anÃ¡lises)

Exemplos: GPT-5, Claude, Gemini, LLaMA

---

## O que sÃ£o redes neurais? {.center}

ğŸ§  Inspiradas no cÃ©rebro humano, redes neurais sÃ£o estruturas matemÃ¡ticas que:

- Recebem **entradas** (textos, imagens, nÃºmeros)
- Processam essas entradas em **camadas** de nÃ³s (neurÃ´nios artificiais)
- Aprendem a **reconhecer padrÃµes** e gerar saÃ­das (respostas)
- Com o tempo, ajustam seus pesos internos para melhorar os resultados.

---

## Deep Learning {.center}

- Conjunto de tÃ©cnicas de **aprendizado de mÃ¡quina** que usa redes neurais com **mÃºltiplas camadas**.  
- Cada camada aprende representaÃ§Ãµes mais complexas dos dados.  
- A â€œprofundidadeâ€ se refere ao **nÃºmero de camadas** de neurÃ´nios artificiais.

---

## Treinamento: como o modelo aprende? {.center}

1. O modelo **prevÃª uma palavra** em uma sequÃªncia.  
2. Compara com a palavra real do corpus.  
3. Calcula o **erro (diferenÃ§a)** entre previsÃ£o e realidade.  
4. Usa um algoritmo de **retropropagaÃ§Ã£o (backpropagation)** para ajustar os pesos.  
5. Repete milhÃµes de vezes atÃ© aprender padrÃµes estÃ¡veis.

âš™ï¸ Essa Ã© a essÃªncia do **aprendizado supervisionado** nas redes neurais profundas.

---

## O que sÃ£o os â€œparÃ¢metrosâ€? {.center}

ğŸ”¢ **ParÃ¢metros = pesos e vieses (biases)** das conexÃµes entre neurÃ´nios.  

- Cada parÃ¢metro define **como uma entrada influencia uma saÃ­da**.  
- Eles sÃ£o **ajustados durante o treinamento** â€” Ã© isso que o modelo realmente â€œaprendeâ€.  

---

## O que sÃ£o os â€œparÃ¢metrosâ€? {.center}

Em termos simples:

> Cada parÃ¢metro Ã© um **nÃºmero** que representa um ajuste aprendido.  
> Quanto mais parÃ¢metros, maior a **capacidade de representar padrÃµes complexos**.

---

## DimensÃ£o dos modelos {.center}

| Modelo | Ano | ParÃ¢metros  |
|:-------|:----:|:----------:|
| GPT-2 | 2019 | 1,5 bilhÃ£o  |
| GPT-3 | 2020 | 175 bilhÃµes |
| GPT-4 | 2023 | â‰ƒ 1 trilhÃ£o |


---

## Principais inovaÃ§Ãµes do GPT-5 {.center}

### 1. Arquitetura tipo â€œMixture-of-Expertsâ€  
- ~52,5 trilhÃµes de parÃ¢metros no total â€” frente aos ~1,76 trilhÃ£o estimados para o GPTâ€‘4.
- NÃ£o â€œtodos os parÃ¢metros disparamâ€ em cada tarefa â€” o sistema ativa sub-modelos especializados conforme o contexto.

---

## Principais inovaÃ§Ãµes do GPT-5 {.center}

### 2. â€œModelo unificadoâ€ para o usuÃ¡rio  
- UsuÃ¡rio deixa de escolher entre vÃ¡rios modelos: o sistema decide â€œmodo rÃ¡pidoâ€ vs â€œmodo raciocÃ­nio profundoâ€. 
- Janela de contexto expandida: ultrapassa **um milhÃ£o de tokens**, o que permite analisar livros inteiros, bases de cÃ³digo ou longas conversas. 

**Fonte:** [ â€œGPT-5: OpenAIâ€™s Unified Intelligence Playâ€, Medium, 7 de ago de 2025. ](https://medium.com/%40cognidownunder/gpt-5-openais-unified-intelligence-play-50fcfab6665b)

---

## Qual o custo disso? {.center}

- AltÃ­ssimo consumo de energia (relaÃ§Ã£o direta com o uso)
- Custos operacionais por **milhÃ£o de tokens gerados**
- Impacto ambiental: Ã¡gua, eletricidades, recursos naturais (mineraÃ§Ã£o para hardware)

âš ï¸ Usar IA generativa tem um custo **invisÃ­vel ao usuÃ¡rio, mas real**

---

## Word Embeddings {.center}

**Como a mÃ¡quina â€œentendeâ€ palavras?**

- Cada palavra Ã© convertida em um **vetor numÃ©rico** (lista de nÃºmeros).  
- Palavras com sentidos semelhantes ficam **prÃ³ximas no espaÃ§o vetorial**.

Exemplo:  
`rei - homem + mulher â‰ˆ rainha`

Essa representaÃ§Ã£o Ã© chamada de **embedding**: um mapa semÃ¢ntico aprendido a partir de contextos linguÃ­sticos.

---

## Transformers {.center}

- Introduzidos por Vaswani et al. (2017): *â€œAttention is All You Needâ€*  
- SubstituÃ­ram redes recorrentes (RNNs) por **mecanismos de atenÃ§Ã£o**.

**AtenÃ§Ã£o** = o modelo decide quais palavras anteriores sÃ£o relevantes para prever a prÃ³xima palavra.

> Isso permite lidar com **longos contextos** e manter **coerÃªncia semÃ¢ntica**.

---

## Como o Transformer funciona? {.center}

Cada camada realiza trÃªs passos:

1. **Codifica** o contexto das palavras anteriores;  
2. **Aplica atenÃ§Ã£o** para focar nas partes mais relevantes;  
3. **Gera prediÃ§Ãµes** ajustadas pela probabilidade.

As camadas se empilham â€” cada uma refina a compreensÃ£o da anterior.  

Resultado: o modelo aprende **estrutura, estilo e contexto** de linguagem.

---

## Fine-Tuning {.center}

ApÃ³s o treinamento inicial, o modelo Ã© **refinado em conjuntos menores de dados**,  
com objetivos especÃ­ficos, como:

- Traduzir textos  
- Gerar cÃ³digo  
- Responder perguntas histÃ³ricas  

---

## Instruction Tuning {.center}

ğŸ“ **Aprendendo a seguir instruÃ§Ãµes**

- Fase em que o modelo Ã© treinado para responder de forma **Ãºtil, clara e segura**.  
- Usa exemplos de perguntas e respostas humanas cuidadosamente anotadas.  
- Ensina o modelo a **seguir comandos (â€œfaÃ§a isso...â€)** em linguagem natural.

---

## ViÃ©ses na IA Generativa {.center}

ğŸ” **Tipos de viÃ©s:**

- **ViÃ©s de contexto**: falha ao interpretar corretamente
- **ViÃ©s de automaÃ§Ã£o**: confianÃ§a cega nas respostas
- **ViÃ©s de representatividade**: grupos sub-representados
- **ViÃ©s de exclusÃ£o**: ausÃªncia total de certos grupos

ğŸ’¡ Causa: dados de treinamento com desigualdades  
âš ï¸ ConsequÃªncia: reproduÃ§Ã£o de discriminaÃ§Ã£o e injustiÃ§as

---

## AlucinaÃ§Ãµes em Modelos de Linguagem {.center}

ğŸ¤¯ **O que Ã© uma alucinaÃ§Ã£o?**

- Quando o modelo **gera uma resposta incorreta ou inventada**, mas com aparÃªncia de verdade.  
- Resulta da forma como ele **estima probabilidades** para a prÃ³xima palavra â€” sem acesso direto Ã  realidade.  
- O modelo nÃ£o â€œsabeâ€, apenas **prediz** o texto mais provÃ¡vel.

---

## AlucinaÃ§Ãµes em Modelos de Linguagem {.center}

ğŸ“Œ **Exemplos de alucinaÃ§Ã£o:**

- CitaÃ§Ãµes inexistentes  
- Fatos histÃ³ricos trocados  
- Nomes ou datas inventadas  

âš ï¸ **Por isso Ã© fundamental:** Verificar fontes, validar resultados e manter **supervisÃ£o humana** no uso acadÃªmico.

---

## Temperatura e Criatividade {.center}

Ã‰ um **parÃ¢metro de controle de aleatoriedade** na geraÃ§Ã£o de texto:

| Valor                    | Efeito                                    |
|:-------------------------|:------------------------------------------|
| ğŸ”¹ **Baixa (â‰ˆ 0.1â€“0.3)** | Respostas mais previsÃ­veis e consistentes |
| ğŸ”¸ **MÃ©dia (â‰ˆ 0.5â€“0.7)** | EquilÃ­brio entre coerÃªncia e criatividade |
| ğŸ”º **Alta (â‰ˆ 0.8â€“1.0+)** | Respostas criativas, mas menos estÃ¡veis   |


---

## Controle da Temperatura {.center}


| Contexto | Pode alterar? | ObservaÃ§Ã£o |
|:----------|:---------------|:------------|
| ğŸ’¬ **Chatbots prontos** (ChatGPT, Gemini, Copilot) | âŒ NÃ£o | Valor prÃ©-definido pelo sistema |
| ğŸ§  **API / cÃ³digo** (`temperature=`) | âœ… Sim | Controla a criatividade e variaÃ§Ã£o das respostas |

---

## Do modelo ao diÃ¡logo {.center}

ğŸ’¬ Quando interagimos com o GPT:

1. O texto Ã© convertido em **tokens** (pedaÃ§os de palavras);  
2. O modelo calcula probabilidades para o prÃ³ximo token;  
3. Gera as palavras uma a uma, **predizendo o texto mais provÃ¡vel**;  
4. MantÃ©m o contexto da conversa por meio de **atenÃ§Ã£o e embeddings**.

ğŸ“ Cada resposta Ã© uma **nova previsÃ£o probabilÃ­stica**, nÃ£o uma busca em banco de dados.

---

## Como se estrutura um chatbot com IA? {.center}

- **Modelo base (LLM)**: entende e gera respostas
- **Interface (chat)**: onde ocorre a interaÃ§Ã£o
- **Regras e filtros**: para seguranÃ§a e adequaÃ§Ã£o
- **MemÃ³ria contextual (opcional)**: para lembrar o que foi dito

---

## Onde rodam os chatbots? A infraestrutura dos LLMs {.center}

ğŸ–¥ï¸ Modelos como o ChatGPT sÃ£o executados em **megaestruturas computacionais** com:

- Milhares de **GPUs especializadas** (ex: NVIDIA A100)
- Ambientes distribuÃ­dos em **data centers de alta performance**
- RefrigeraÃ§Ã£o avanÃ§ada e uso intensivo de energia

---

## Modelos Fechados vs Modelos Abertos {.columns}

::: {.column width="50%"}
**ğŸ”’ Modelos Fechados**

- Desenvolvidos por grandes empresas (OpenAI, Google)
- Acesso via APIs
- Pouca transparÃªncia (_black box_)
- Exemplos: GPT-5, Claude, Gemini
:::

::: {.column width="50%"}
**ğŸ”“ Modelos Abertos**

- CÃ³digo-fonte acessÃ­vel e personalizÃ¡vel
- ExecutÃ¡veis localmente
- Maior controle e privacidade
- Exemplos: LLaMA (Meta), Mistral, Sabia-7B 
:::

---

## Como escolher entre modelos? {.center}

::: {.column width="48%"}
ğŸ“Œ **Depende de:**

- âš™ï¸ Infraestrutura disponÃ­vel
- ğŸ›¡ï¸ Requisitos de seguranÃ§a e privacidade
- ğŸ¯ NÃ­vel de personalizaÃ§Ã£o desejado
- ğŸ’° Custo e acessibilidade
:::

::: {.column width="4%"}
:::

::: {.column width="48%"}
ğŸ“ **Modelos abertos sÃ£o preferÃ­veis quando hÃ¡:**

âœ”ï¸ Necessidade de controle  
âœ”ï¸ Regras rÃ­gidas de privacidade  
âœ”ï¸ Desejo de transparÃªncia e auditabilidade
:::

---

## 3. Rodando modelos locais com Ollama {.center}

---

## O que Ã© o Ollama? {.columns}

::: {.column width="65%"}
- Plataforma para **executar Modelos de Linguagem (LLMs)** localmente  
- Suporte nativo a modelos **abertos**   
- Funciona em **Linux**, **Windows** e **macOS**  
- Usa containers de modelos via **Modelfile** (que possibilita customizar diferentes aspectos do modelo)
:::

::: {.column width="5%"}
:::

::: {.column width="30%"}
![](https://ollama.com/public/ollama.png)
:::

---

## Por que usar o Ollama? {.center}

- ğŸ”’ **Privacidade:** ideal para dados sensÃ­veis e pesquisa acadÃªmica
- âš–ï¸ **Autonomia e soberania digital**  
- ğŸ§ª **Reprodutibilidade:** sempre a mesma versÃ£o local  
- ğŸª¶ **Modelos leves:** funcionam em laptops modestos  
- âš™ï¸ **IntegraÃ§Ã£o fÃ¡cil** com Python, Open WebUi, LangChain, etc.
- ğŸ‘©â€ğŸ« Aprendizado prÃ¡tico sobre LLMs e IA Generativa

---

## InstalaÃ§Ã£o {.center}

### ğŸ§ Linux (Ubuntu)

```bash
curl -fsSL https://ollama.com/install.sh | sh
````

### ğŸªŸ Windows 

Baixe o instalador em:

[https://ollama.com/download/windows](https://ollama.com/download/windows)

---

## Conferindo sua instalaÃ§Ã£o {.center}

```bash
# Verificar se o serviÃ§o estÃ¡ ativo
systemctl status ollama

# Listar modelos instalados
ollama list
```

---

## Baixando modelos {.center}

```bash
ollama pull phi3:mini
ollama pull gemma3:4b
ollama pull qwen3:0.6b
```

ğŸ“Œ *Dica:* Prefira modelos **small/mini** em PCs sem GPU.

---

## Rodando seu primeiro modelo {.center}

```bash
ollama run phi3:mini
```

Depois basta digitar um prompt:

```
Explique o conceito de â€œarquivo histÃ³ricoâ€ em 3 linhas.
```

---

## Usando o Ollama com prompts diretos {.center}

```bash
ollama run mistral "Resuma este texto em 2 linhas: ... "
```

---

## Usando via API (Python) {.center}

```python
import requests

r = requests.post(
  "http://localhost:11434/api/generate",
  json={
    "model": "phi3:mini",
    "prompt": "Explique RAG em 3 linhas."
  }
)

print(r.json()["response"])
```

---

## Onde ficam os modelos? {.center}

```bash
~/.ollama/models
```

No windows:

```bash
%USERPROFILE%\.ollama\models
```

---

## Estrutura dos modelos no Ollama {.center}

Cada modelo Ã© um **container** com:

* arquitetura
* quantizaÃ§Ã£o
* contexto
* parÃ¢metros especiais
* instruÃ§Ãµes do Modelfile

---

## Modelfile: criando seu prÃ³prio modelo local {.columns}

Um *Modelfile* Ã© um arquivo de configuraÃ§Ã£o usado pelo **Ollama** para:

- Criar uma **variante personalizada** de um modelo;
- Definir **instruÃ§Ãµes iniciais** (estilo, persona, tom);
- Ajustar **parÃ¢metros bÃ¡sicos** (contexto, temperatura, template);
- Incluir **exemplos** e **comportamentos padrÃ£o**.

---

### ğŸ“ Exemplo de `Modelfile` {.center}

```bash
FROM phi3:mini

# InstruÃ§Ãµes que o modelo seguirÃ¡ sempre
SYSTEM """
VocÃª Ã© um assistente especializado em **resumo, sÃ­ntese e anÃ¡lise de textos**, com foco em Humanidades, CiÃªncias Sociais e Ã¡reas correlatas.

OBJETIVO PRINCIPAL
- Transformar textos longos em **resumos claros, fiÃ©is, estruturados e concisos**, mantendo precisÃ£o conceitual.
- Priorizar **ideias centrais**, conceitos, argumentos, mÃ©todos e conclusÃµes.
- Reduzir redundÃ¢ncias, cortar ruÃ­do e destacar relaÃ§Ãµes importantes entre conceitos.

IDIOMA E ESTILO
- Responda SEMPRE em portuguÃªs do Brasil (exceto nomes de pacotes, comandos, APIs e cÃ³digo).
- Seja direto, tÃ©cnico, claro e objetivo.
- Evite prolixidade. Prefira frases curtas.
- Quando apropriado, ofereÃ§a:
  - Resumo estrutural (tÃ³picos)
  - Resumo narrativo (parÃ¡grafo)
  - VersÃ£o ultracurta (1â€“2 frases)

REGRAS DE RESUMO
- NÃ£o inventar informaÃ§Ãµes.
- NÃ£o fazer interpretaÃ§Ã£o alÃ©m do texto, exceto quando solicitado.
- Se o texto estiver confuso, reorganize-o com coerÃªncia.
- Se o usuÃ¡rio nÃ£o fornecer o texto, peÃ§a que envie o conteÃºdo.
"""

# ConfiguraÃ§Ãµes bÃ¡sicas
PARAMETER temperature 0.2
PARAMETER num_ctx 8000
```

---

## Como construir e usar um Modelfile {.center}

```bash
# Criar o modelo personalizado
ollama create assistente -f Modelfile

# Rodar o modelo
ollama run assistente
```

ğŸ“Œ *Resultado:* vocÃª cria uma **versÃ£o local** e **sob medida** do modelo, ideal para pesquisa, ensino e projetos repetitivos.

---

## Modelos em Nuvem do Ollama {.center}

### O que sÃ£o?

* Modelos executados **na infraestrutura da Ollama**
* NÃ£o dependem do hardware local (CPU/GPU)
* Permitem usar **modelos maiores** e **janelas de contexto ampliadas**

---

## Modelos em Nuvem do Ollama {.center}

![Privacidade](imgs/privacy.png)

---

## Modelos em Nuvem do Ollama {.center}

### Por que usar?

* Computadores pessoais sem GPU
* Maior desempenho
* Modelos muito grandes (ex.: `qwen3-vl:235b-cloud`, `gpt-oss:120b-cloud`)
* Menor consumo de energia local

---

## Modelos em Nuvem do Ollama {.center}

### CaracterÃ­sticas

* Requer **API Key**
* Limites de uso e uso cobrado por processamento (dependendo do plano)
* Igual sintaxe dos modelos locais no OpenAI-compatible endpoint
* IntegraÃ§Ã£o simples com ferramentas existentes

---

## Gerando sua API Key da Ollama {.center}

### Antes de tudo: faÃ§a login no Ollama {.center}

1. VÃ¡ para:
   **[https://ollama.com/](https://ollama.com/)**
2. Clique em **Sign In** ou **Create Account**
3. FaÃ§a login normalmente (e-mail + senha)

---

## Fazer login pelo terminal {.center}

Depois de logar no site, abra seu terminal e execute:

```bash
ollama signin
```

VocÃª verÃ¡ uma mensagem pedindo autorizaÃ§Ã£o no navegador.
Acesse o link exibido e confirme o login.

---

## Gerando sua API Key da Ollama {.center}

1. Acesse
   **[https://ollama.com/settings/keys](https://ollama.com/settings/keys)**
2. Clique em **Create API Key**
3. Defina um nome (ex.: *oficina-2shd*)
4. Clique em **Create**
5. **Copie e salve a chave**
   * Ela nÃ£o aparecerÃ¡ novamente

---

## Gerando sua API Key da Ollama {.center}

6. Configure no sistema:

**Linux / macOS**

```bash
export OLLAMA_API_KEY="SUA_CHAVE_AQUI"
```

**Windows (PowerShell)**

```powershell
setx OLLAMA_API_KEY "SUA_CHAVE_AQUI"
```

---

## Gerando sua API Key da Ollama {.center}

### ObservaÃ§Ãµes importantes

* Sem a API Key, modelos â€œ*-cloudâ€ nÃ£o funcionam
* A chave permite autenticaÃ§Ã£o no endpoint da Ollama Cloud
* NÃ£o compartilhe sua chave â€” trate como senha
* VocÃª pode revogar uma chave a qualquer momento

---

## Atividade prÃ¡tica {.center}

ğŸ”§ **Tarefas:**

1. Rodar **2 modelos locais**
   (Phi-3, Mistral, LLaMA3, Gemma, SabiÃ¡)

2. [ Criar um Modelfile simples para resumo acadÃªmico ](https://github.com/ericbrasiln/2shdufba/blob/main/ollama_modelfile/Modelfile)

3. [Criar um script Python que receba](https://github.com/ericbrasiln/2shdufba/tree/main/ollama_python):

   * 1 arquivo `.txt`
   * 1 instruÃ§Ã£o
   * Retorne anÃ¡lise ou resumo

---

## Cuidados e Ã©tica {.center}

* Sempre revisar conteÃºdos gerados
* Modelos locais **nÃ£o substituem mÃ©todo cientÃ­fico**
* Documentar o uso de IA no trabalho
* Checar viÃ©ses, alucinaÃ§Ãµes e limitaÃ§Ãµes
* Preferir modelos **abertos e auditÃ¡veis**

